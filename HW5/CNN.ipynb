{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "In this notebook, we'll put together our convolutional layers to implement a 3-layer CNN.  Then, we'll ask you to implement a CNN that can achieve > 65% validation error on CIFAR-10.\n",
    "\n",
    "CS231n has built a solid API for building these modular frameworks and training them, and we will use their very well implemented framework as opposed to \"reinventing the wheel.\"  This includes using their Solver, various utility functions, their layer structure, and their implementation of fast CNN layers.  This also includes nndl.fc_net, nndl.layers, and nndl.layer_utils.  As in prior assignments, we thank Serena Yeung & Justin Johnson for permission to use code written for the CS 231n class (cs231n.stanford.edu).  \n",
    "\n",
    "\n",
    "If you have not completed the Spatial BatchNorm Notebook, please see the following description from that notebook:\n",
    "\n",
    "Please copy and paste your prior implemented code from HW #4 to start this assignment.  If you did not correctly implement the layers in HW #4, you may collaborate with a classmate to use their layer implementations from HW #4.  You may also visit TA or Prof OH to correct your implementation.  \n",
    "\n",
    "You'll want to copy and paste from HW #4:\n",
    "    - layers.py for your FC network layers, as well as batchnorm and dropout.\n",
    "    - layer_utils.py for your combined FC network layers.\n",
    "    - optim.py for your optimizers.\n",
    "\n",
    "Be sure to place these in the `nndl/` directory so they're imported correctly.  Note, as announced in class, we will not be releasing our solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sujitsilas/Desktop/UCLA/Winter 2025/EE ENGR 247/Homeworks/HW5\n"
     ]
    }
   ],
   "source": [
    "cd /Users/sujitsilas/Desktop/UCLA/Winter 2025/EE ENGR 247/Homeworks/HW5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nndl.cnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from nndl.layers import *\n",
    "from nndl.conv_layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32) \n",
      "y_train: (49000,) \n",
      "X_val: (1000, 3, 32, 32) \n",
      "y_val: (1000,) \n",
      "X_test: (1000, 3, 32, 32) \n",
      "y_test: (1000,) \n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k in data.keys():\n",
    "  print('{}: {} '.format(k, data[k].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three layer CNN\n",
    "\n",
    "In this notebook, you will implement a three layer CNN.  The `ThreeLayerConvNet` class is in `nndl/cnn.py`.  You'll need to modify that code for this section, including the initialization, as well as the calculation of the loss and gradients.  You should be able to use the building blocks you have either earlier coded or that we have provided.  Be sure to use the fast layers.\n",
    "\n",
    "The architecture of this CNN will be:\n",
    "\n",
    "conv - relu - 2x2 max pool - affine - relu - affine - softmax\n",
    "\n",
    "We won't use batchnorm yet.  You've also done enough of these to know how to debug; use the cells below.\n",
    "\n",
    "Note: As we are implementing several layers CNN networks. The gradient error can be expected for the `eval_numerical_gradient()` function. If your `W1 max relative error` and `W2 max relative error` are around or below 0.01, they should be acceptable. Other errors should be less than 1e-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 0.0003876121992847651\n",
      "W2 max relative error: 0.006087720019066434\n",
      "W3 max relative error: 0.00015610239462995478\n",
      "b1 max relative error: 3.0482494482486916e-05\n",
      "b2 max relative error: 6.380458561304692e-07\n",
      "b3 max relative error: 1.3344205311457414e-09\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('{} max relative error: {}'.format(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit small dataset\n",
    "\n",
    "To check your CNN implementation, let's overfit a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 20) loss: 2.401445\n",
      "(Epoch 0 / 10) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 2 / 20) loss: 3.384418\n",
      "(Epoch 1 / 10) train acc: 0.240000; val_acc: 0.133000\n",
      "(Iteration 3 / 20) loss: 3.716092\n",
      "(Iteration 4 / 20) loss: 2.159221\n",
      "(Epoch 2 / 10) train acc: 0.350000; val_acc: 0.133000\n",
      "(Iteration 5 / 20) loss: 2.267118\n",
      "(Iteration 6 / 20) loss: 2.031910\n",
      "(Epoch 3 / 10) train acc: 0.320000; val_acc: 0.138000\n",
      "(Iteration 7 / 20) loss: 1.877286\n",
      "(Iteration 8 / 20) loss: 2.123628\n",
      "(Epoch 4 / 10) train acc: 0.450000; val_acc: 0.162000\n",
      "(Iteration 9 / 20) loss: 1.535895\n",
      "(Iteration 10 / 20) loss: 1.580020\n",
      "(Epoch 5 / 10) train acc: 0.490000; val_acc: 0.142000\n",
      "(Iteration 11 / 20) loss: 1.679795\n",
      "(Iteration 12 / 20) loss: 1.332821\n",
      "(Epoch 6 / 10) train acc: 0.590000; val_acc: 0.194000\n",
      "(Iteration 13 / 20) loss: 1.275423\n",
      "(Iteration 14 / 20) loss: 1.504342\n",
      "(Epoch 7 / 10) train acc: 0.680000; val_acc: 0.218000\n",
      "(Iteration 15 / 20) loss: 0.789805\n",
      "(Iteration 16 / 20) loss: 1.229985\n",
      "(Epoch 8 / 10) train acc: 0.670000; val_acc: 0.176000\n",
      "(Iteration 17 / 20) loss: 1.163397\n",
      "(Iteration 18 / 20) loss: 1.111179\n",
      "(Epoch 9 / 10) train acc: 0.780000; val_acc: 0.214000\n",
      "(Iteration 19 / 20) loss: 0.869874\n",
      "(Iteration 20 / 20) loss: 0.715722\n",
      "(Epoch 10 / 10) train acc: 0.800000; val_acc: 0.226000\n"
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"orientation\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"dpi\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"facecolor\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"edgecolor\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"bbox_inches_restore\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "\n",
    "Now we train the 3 layer CNN on CIFAR-10 and assess its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 980) loss: 2.304777\n",
      "(Epoch 0 / 1) train acc: 0.085000; val_acc: 0.088000\n",
      "(Iteration 21 / 980) loss: 2.343098\n",
      "(Iteration 41 / 980) loss: 2.053573\n",
      "(Iteration 61 / 980) loss: 2.170969\n",
      "(Iteration 81 / 980) loss: 2.136866\n",
      "(Iteration 101 / 980) loss: 1.953034\n",
      "(Iteration 121 / 980) loss: 1.788268\n",
      "(Iteration 141 / 980) loss: 1.951241\n",
      "(Iteration 161 / 980) loss: 1.913286\n",
      "(Iteration 181 / 980) loss: 1.729100\n",
      "(Iteration 201 / 980) loss: 2.337749\n",
      "(Iteration 221 / 980) loss: 1.633811\n",
      "(Iteration 241 / 980) loss: 1.592094\n",
      "(Iteration 261 / 980) loss: 1.806649\n",
      "(Iteration 281 / 980) loss: 1.571482\n",
      "(Iteration 301 / 980) loss: 1.605426\n",
      "(Iteration 321 / 980) loss: 1.563820\n",
      "(Iteration 341 / 980) loss: 1.562186\n",
      "(Iteration 361 / 980) loss: 1.700869\n",
      "(Iteration 381 / 980) loss: 1.618070\n",
      "(Iteration 401 / 980) loss: 1.490276\n",
      "(Iteration 421 / 980) loss: 1.642910\n",
      "(Iteration 441 / 980) loss: 1.657358\n",
      "(Iteration 461 / 980) loss: 1.583289\n",
      "(Iteration 481 / 980) loss: 1.842153\n",
      "(Iteration 501 / 980) loss: 1.744576\n",
      "(Iteration 521 / 980) loss: 1.808263\n",
      "(Iteration 541 / 980) loss: 1.774324\n",
      "(Iteration 561 / 980) loss: 1.470594\n",
      "(Iteration 581 / 980) loss: 1.354535\n",
      "(Iteration 601 / 980) loss: 1.653459\n",
      "(Iteration 621 / 980) loss: 1.496483\n",
      "(Iteration 641 / 980) loss: 1.422213\n",
      "(Iteration 661 / 980) loss: 1.665424\n",
      "(Iteration 681 / 980) loss: 1.421259\n",
      "(Iteration 701 / 980) loss: 1.654716\n",
      "(Iteration 721 / 980) loss: 1.534159\n",
      "(Iteration 741 / 980) loss: 1.509008\n",
      "(Iteration 761 / 980) loss: 1.222114\n",
      "(Iteration 781 / 980) loss: 1.609916\n",
      "(Iteration 801 / 980) loss: 1.381218\n",
      "(Iteration 821 / 980) loss: 1.603093\n",
      "(Iteration 841 / 980) loss: 1.563135\n",
      "(Iteration 861 / 980) loss: 1.488229\n",
      "(Iteration 881 / 980) loss: 1.534100\n",
      "(Iteration 901 / 980) loss: 1.640235\n",
      "(Iteration 921 / 980) loss: 1.680015\n",
      "(Iteration 941 / 980) loss: 1.544884\n",
      "(Iteration 961 / 980) loss: 1.558549\n",
      "(Epoch 1 / 1) train acc: 0.457000; val_acc: 0.479000\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=1, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get > 65% validation accuracy on CIFAR-10.\n",
    "\n",
    "In the last part of the assignment, we'll now ask you to train a CNN to get better than 65% validation accuracy on CIFAR-10.\n",
    "\n",
    "### Things you should try:\n",
    "- Filter size: Above we used 7x7; but VGGNet and onwards showed stacks of 3x3 filters are good.  \n",
    "- Number of filters: Above we used 32 filters. Do more or fewer do better?\n",
    "- Batch normalization: Try adding spatial batch normalization after convolution layers and vanilla batch normalization aafter affine layers. Do your networks train faster?\n",
    "- Network architecture: Can a deeper CNN do better?  Consider these architectures:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]XN - [affine]XM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32) \n",
      "y_train: (49000,) \n",
      "X_val: (1000, 3, 32, 32) \n",
      "y_val: (1000,) \n",
      "X_test: (1000, 3, 32, 32) \n",
      "y_test: (1000,) \n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 24460) loss: 2.306876\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.119000\n",
      "(Iteration 21 / 24460) loss: 1.968556\n",
      "(Iteration 41 / 24460) loss: 1.992259\n",
      "(Iteration 61 / 24460) loss: 1.876604\n",
      "(Iteration 81 / 24460) loss: 1.597667\n",
      "(Iteration 101 / 24460) loss: 1.623970\n",
      "(Iteration 121 / 24460) loss: 1.846559\n",
      "(Iteration 141 / 24460) loss: 1.563914\n",
      "(Iteration 161 / 24460) loss: 1.742950\n",
      "(Iteration 181 / 24460) loss: 1.841568\n",
      "(Iteration 201 / 24460) loss: 1.713142\n",
      "(Iteration 221 / 24460) loss: 1.522989\n",
      "(Iteration 241 / 24460) loss: 1.362856\n",
      "(Iteration 261 / 24460) loss: 1.529097\n",
      "(Iteration 281 / 24460) loss: 1.761435\n",
      "(Iteration 301 / 24460) loss: 1.737436\n",
      "(Iteration 321 / 24460) loss: 1.818836\n",
      "(Iteration 341 / 24460) loss: 1.487090\n",
      "(Iteration 361 / 24460) loss: 1.521646\n",
      "(Iteration 381 / 24460) loss: 1.424549\n",
      "(Iteration 401 / 24460) loss: 1.651834\n",
      "(Iteration 421 / 24460) loss: 1.385865\n",
      "(Iteration 441 / 24460) loss: 1.480200\n",
      "(Iteration 461 / 24460) loss: 1.645105\n",
      "(Iteration 481 / 24460) loss: 1.579155\n",
      "(Iteration 501 / 24460) loss: 1.520588\n",
      "(Iteration 521 / 24460) loss: 1.647415\n",
      "(Iteration 541 / 24460) loss: 1.642582\n",
      "(Iteration 561 / 24460) loss: 1.405780\n",
      "(Iteration 581 / 24460) loss: 1.372605\n",
      "(Iteration 601 / 24460) loss: 1.690252\n",
      "(Iteration 621 / 24460) loss: 1.316471\n",
      "(Iteration 641 / 24460) loss: 1.303148\n",
      "(Iteration 661 / 24460) loss: 1.605781\n",
      "(Iteration 681 / 24460) loss: 1.452811\n",
      "(Iteration 701 / 24460) loss: 1.363777\n",
      "(Iteration 721 / 24460) loss: 1.608391\n",
      "(Iteration 741 / 24460) loss: 1.441291\n",
      "(Iteration 761 / 24460) loss: 1.141424\n",
      "(Iteration 781 / 24460) loss: 1.375796\n",
      "(Iteration 801 / 24460) loss: 1.325216\n",
      "(Iteration 821 / 24460) loss: 1.460260\n",
      "(Iteration 841 / 24460) loss: 1.398989\n",
      "(Iteration 861 / 24460) loss: 1.490719\n",
      "(Iteration 881 / 24460) loss: 1.457513\n",
      "(Iteration 901 / 24460) loss: 1.390862\n",
      "(Iteration 921 / 24460) loss: 1.437478\n",
      "(Iteration 941 / 24460) loss: 1.466623\n",
      "(Iteration 961 / 24460) loss: 1.361435\n",
      "(Iteration 981 / 24460) loss: 1.292501\n",
      "(Iteration 1001 / 24460) loss: 1.578588\n",
      "(Iteration 1021 / 24460) loss: 1.355552\n",
      "(Iteration 1041 / 24460) loss: 1.280713\n",
      "(Iteration 1061 / 24460) loss: 1.468659\n",
      "(Iteration 1081 / 24460) loss: 1.543792\n",
      "(Iteration 1101 / 24460) loss: 1.382876\n",
      "(Iteration 1121 / 24460) loss: 1.272412\n",
      "(Iteration 1141 / 24460) loss: 1.097616\n",
      "(Iteration 1161 / 24460) loss: 1.353857\n",
      "(Iteration 1181 / 24460) loss: 1.581810\n",
      "(Iteration 1201 / 24460) loss: 1.285922\n",
      "(Iteration 1221 / 24460) loss: 1.488349\n",
      "(Epoch 1 / 20) train acc: 0.561000; val_acc: 0.576000\n",
      "(Iteration 1241 / 24460) loss: 1.347696\n",
      "(Iteration 1261 / 24460) loss: 1.356189\n",
      "(Iteration 1281 / 24460) loss: 1.454577\n",
      "(Iteration 1301 / 24460) loss: 1.384634\n",
      "(Iteration 1321 / 24460) loss: 1.067988\n",
      "(Iteration 1341 / 24460) loss: 1.311321\n",
      "(Iteration 1361 / 24460) loss: 1.319083\n",
      "(Iteration 1381 / 24460) loss: 1.300423\n",
      "(Iteration 1401 / 24460) loss: 1.204024\n",
      "(Iteration 1421 / 24460) loss: 1.580231\n",
      "(Iteration 1441 / 24460) loss: 1.349325\n",
      "(Iteration 1461 / 24460) loss: 1.583886\n",
      "(Iteration 1481 / 24460) loss: 1.513777\n",
      "(Iteration 1501 / 24460) loss: 1.145278\n",
      "(Iteration 1521 / 24460) loss: 1.128353\n",
      "(Iteration 1541 / 24460) loss: 1.222516\n",
      "(Iteration 1561 / 24460) loss: 1.456970\n",
      "(Iteration 1581 / 24460) loss: 1.178496\n",
      "(Iteration 1601 / 24460) loss: 1.266458\n",
      "(Iteration 1621 / 24460) loss: 1.389638\n",
      "(Iteration 1641 / 24460) loss: 1.363404\n",
      "(Iteration 1661 / 24460) loss: 1.229295\n",
      "(Iteration 1681 / 24460) loss: 1.256337\n",
      "(Iteration 1701 / 24460) loss: 1.221898\n",
      "(Iteration 1721 / 24460) loss: 1.338768\n",
      "(Iteration 1741 / 24460) loss: 1.426326\n",
      "(Iteration 1761 / 24460) loss: 1.540652\n",
      "(Iteration 1781 / 24460) loss: 1.330854\n",
      "(Iteration 1801 / 24460) loss: 1.185674\n",
      "(Iteration 1821 / 24460) loss: 1.295520\n",
      "(Iteration 1841 / 24460) loss: 1.036330\n",
      "(Iteration 1861 / 24460) loss: 1.395794\n",
      "(Iteration 1881 / 24460) loss: 1.137590\n",
      "(Iteration 1901 / 24460) loss: 1.361948\n",
      "(Iteration 1921 / 24460) loss: 1.181079\n",
      "(Iteration 1941 / 24460) loss: 1.294370\n",
      "(Iteration 1961 / 24460) loss: 1.530800\n",
      "(Iteration 1981 / 24460) loss: 1.239763\n",
      "(Iteration 2001 / 24460) loss: 1.253328\n",
      "(Iteration 2021 / 24460) loss: 1.112606\n",
      "(Iteration 2041 / 24460) loss: 1.173756\n",
      "(Iteration 2061 / 24460) loss: 1.101437\n",
      "(Iteration 2081 / 24460) loss: 1.080073\n",
      "(Iteration 2101 / 24460) loss: 1.261534\n",
      "(Iteration 2121 / 24460) loss: 1.081296\n",
      "(Iteration 2141 / 24460) loss: 1.172701\n",
      "(Iteration 2161 / 24460) loss: 1.054296\n",
      "(Iteration 2181 / 24460) loss: 1.395740\n",
      "(Iteration 2201 / 24460) loss: 1.146489\n",
      "(Iteration 2221 / 24460) loss: 1.294128\n",
      "(Iteration 2241 / 24460) loss: 1.176894\n",
      "(Iteration 2261 / 24460) loss: 1.279096\n",
      "(Iteration 2281 / 24460) loss: 1.151579\n",
      "(Iteration 2301 / 24460) loss: 1.221663\n",
      "(Iteration 2321 / 24460) loss: 1.276504\n",
      "(Iteration 2341 / 24460) loss: 1.039846\n",
      "(Iteration 2361 / 24460) loss: 1.185696\n",
      "(Iteration 2381 / 24460) loss: 1.316065\n",
      "(Iteration 2401 / 24460) loss: 1.084212\n",
      "(Iteration 2421 / 24460) loss: 1.271406\n",
      "(Iteration 2441 / 24460) loss: 1.249542\n",
      "(Epoch 2 / 20) train acc: 0.624000; val_acc: 0.598000\n",
      "(Iteration 2461 / 24460) loss: 1.056764\n",
      "(Iteration 2481 / 24460) loss: 1.282084\n",
      "(Iteration 2501 / 24460) loss: 1.431391\n",
      "(Iteration 2521 / 24460) loss: 1.549483\n",
      "(Iteration 2541 / 24460) loss: 1.354745\n",
      "(Iteration 2561 / 24460) loss: 1.156140\n",
      "(Iteration 2581 / 24460) loss: 1.264956\n",
      "(Iteration 2601 / 24460) loss: 1.075385\n",
      "(Iteration 2621 / 24460) loss: 1.095617\n",
      "(Iteration 2641 / 24460) loss: 1.016367\n",
      "(Iteration 2661 / 24460) loss: 1.391973\n",
      "(Iteration 2681 / 24460) loss: 1.227830\n",
      "(Iteration 2701 / 24460) loss: 1.097501\n",
      "(Iteration 2721 / 24460) loss: 1.225333\n",
      "(Iteration 2741 / 24460) loss: 1.290703\n",
      "(Iteration 2761 / 24460) loss: 1.167535\n",
      "(Iteration 2781 / 24460) loss: 1.198672\n",
      "(Iteration 2801 / 24460) loss: 1.058708\n",
      "(Iteration 2821 / 24460) loss: 1.189990\n",
      "(Iteration 2841 / 24460) loss: 1.214469\n",
      "(Iteration 2861 / 24460) loss: 1.043834\n",
      "(Iteration 2881 / 24460) loss: 1.359238\n",
      "(Iteration 2901 / 24460) loss: 1.088626\n",
      "(Iteration 2921 / 24460) loss: 1.118719\n",
      "(Iteration 2941 / 24460) loss: 1.278219\n",
      "(Iteration 2961 / 24460) loss: 1.171085\n",
      "(Iteration 2981 / 24460) loss: 1.377725\n",
      "(Iteration 3001 / 24460) loss: 1.096921\n",
      "(Iteration 3021 / 24460) loss: 1.144068\n",
      "(Iteration 3041 / 24460) loss: 1.249327\n",
      "(Iteration 3061 / 24460) loss: 1.220553\n",
      "(Iteration 3081 / 24460) loss: 1.436901\n",
      "(Iteration 3101 / 24460) loss: 1.316091\n",
      "(Iteration 3121 / 24460) loss: 1.282944\n",
      "(Iteration 3141 / 24460) loss: 1.029098\n",
      "(Iteration 3161 / 24460) loss: 1.273283\n",
      "(Iteration 3181 / 24460) loss: 1.393962\n",
      "(Iteration 3201 / 24460) loss: 1.269004\n",
      "(Iteration 3221 / 24460) loss: 0.989027\n",
      "(Iteration 3241 / 24460) loss: 1.144613\n",
      "(Iteration 3261 / 24460) loss: 1.158524\n",
      "(Iteration 3281 / 24460) loss: 1.483371\n",
      "(Iteration 3301 / 24460) loss: 1.160021\n",
      "(Iteration 3321 / 24460) loss: 1.047326\n",
      "(Iteration 3341 / 24460) loss: 1.227516\n",
      "(Iteration 3361 / 24460) loss: 1.282071\n",
      "(Iteration 3381 / 24460) loss: 1.194852\n",
      "(Iteration 3401 / 24460) loss: 1.102651\n",
      "(Iteration 3421 / 24460) loss: 1.039087\n",
      "(Iteration 3441 / 24460) loss: 1.245268\n",
      "(Iteration 3461 / 24460) loss: 1.094977\n",
      "(Iteration 3481 / 24460) loss: 1.272504\n",
      "(Iteration 3501 / 24460) loss: 1.137746\n",
      "(Iteration 3521 / 24460) loss: 1.073925\n",
      "(Iteration 3541 / 24460) loss: 1.274178\n",
      "(Iteration 3561 / 24460) loss: 1.123111\n",
      "(Iteration 3581 / 24460) loss: 1.016315\n",
      "(Iteration 3601 / 24460) loss: 0.977294\n",
      "(Iteration 3621 / 24460) loss: 1.109418\n",
      "(Iteration 3641 / 24460) loss: 1.079412\n",
      "(Iteration 3661 / 24460) loss: 1.306691\n",
      "(Epoch 3 / 20) train acc: 0.615000; val_acc: 0.606000\n",
      "(Iteration 3681 / 24460) loss: 1.186673\n",
      "(Iteration 3701 / 24460) loss: 1.188733\n",
      "(Iteration 3721 / 24460) loss: 0.961573\n",
      "(Iteration 3741 / 24460) loss: 1.182203\n",
      "(Iteration 3761 / 24460) loss: 1.123920\n",
      "(Iteration 3781 / 24460) loss: 1.187859\n",
      "(Iteration 3801 / 24460) loss: 1.249436\n",
      "(Iteration 3821 / 24460) loss: 1.327765\n",
      "(Iteration 3841 / 24460) loss: 1.069149\n",
      "(Iteration 3861 / 24460) loss: 1.086173\n",
      "(Iteration 3881 / 24460) loss: 1.120461\n",
      "(Iteration 3901 / 24460) loss: 1.229361\n",
      "(Iteration 3921 / 24460) loss: 0.971813\n",
      "(Iteration 3941 / 24460) loss: 0.980358\n",
      "(Iteration 3961 / 24460) loss: 1.176503\n",
      "(Iteration 3981 / 24460) loss: 1.167497\n",
      "(Iteration 4001 / 24460) loss: 1.065919\n",
      "(Iteration 4021 / 24460) loss: 1.317747\n",
      "(Iteration 4041 / 24460) loss: 1.386604\n",
      "(Iteration 4061 / 24460) loss: 1.151038\n",
      "(Iteration 4081 / 24460) loss: 1.364121\n",
      "(Iteration 4101 / 24460) loss: 1.012557\n",
      "(Iteration 4121 / 24460) loss: 1.030119\n",
      "(Iteration 4141 / 24460) loss: 1.195229\n",
      "(Iteration 4161 / 24460) loss: 1.269469\n",
      "(Iteration 4181 / 24460) loss: 1.139683\n",
      "(Iteration 4201 / 24460) loss: 1.122579\n",
      "(Iteration 4221 / 24460) loss: 1.192275\n",
      "(Iteration 4241 / 24460) loss: 1.252986\n",
      "(Iteration 4261 / 24460) loss: 1.135524\n",
      "(Iteration 4281 / 24460) loss: 1.283747\n",
      "(Iteration 4301 / 24460) loss: 1.270721\n",
      "(Iteration 4321 / 24460) loss: 1.186552\n",
      "(Iteration 4341 / 24460) loss: 1.268468\n",
      "(Iteration 4361 / 24460) loss: 1.076209\n",
      "(Iteration 4381 / 24460) loss: 1.266939\n",
      "(Iteration 4401 / 24460) loss: 1.132033\n",
      "(Iteration 4421 / 24460) loss: 1.187137\n",
      "(Iteration 4441 / 24460) loss: 1.124039\n",
      "(Iteration 4461 / 24460) loss: 1.037668\n",
      "(Iteration 4481 / 24460) loss: 0.946847\n",
      "(Iteration 4501 / 24460) loss: 1.302835\n",
      "(Iteration 4521 / 24460) loss: 1.358413\n",
      "(Iteration 4541 / 24460) loss: 1.114341\n",
      "(Iteration 4561 / 24460) loss: 1.211226\n",
      "(Iteration 4581 / 24460) loss: 1.100552\n",
      "(Iteration 4601 / 24460) loss: 1.133334\n",
      "(Iteration 4621 / 24460) loss: 1.033083\n",
      "(Iteration 4641 / 24460) loss: 0.888846\n",
      "(Iteration 4661 / 24460) loss: 1.051547\n",
      "(Iteration 4681 / 24460) loss: 1.117359\n",
      "(Iteration 4701 / 24460) loss: 1.271176\n",
      "(Iteration 4721 / 24460) loss: 1.424554\n",
      "(Iteration 4741 / 24460) loss: 0.966229\n",
      "(Iteration 4761 / 24460) loss: 1.128299\n",
      "(Iteration 4781 / 24460) loss: 1.385073\n",
      "(Iteration 4801 / 24460) loss: 1.141072\n",
      "(Iteration 4821 / 24460) loss: 1.082224\n",
      "(Iteration 4841 / 24460) loss: 1.258802\n",
      "(Iteration 4861 / 24460) loss: 1.053847\n",
      "(Iteration 4881 / 24460) loss: 1.013378\n",
      "(Epoch 4 / 20) train acc: 0.673000; val_acc: 0.643000\n",
      "(Iteration 4901 / 24460) loss: 1.133238\n",
      "(Iteration 4921 / 24460) loss: 1.126092\n",
      "(Iteration 4941 / 24460) loss: 1.502725\n",
      "(Iteration 4961 / 24460) loss: 1.156968\n",
      "(Iteration 4981 / 24460) loss: 1.049297\n",
      "(Iteration 5001 / 24460) loss: 1.215352\n",
      "(Iteration 5021 / 24460) loss: 0.973151\n",
      "(Iteration 5041 / 24460) loss: 1.231094\n",
      "(Iteration 5061 / 24460) loss: 1.261416\n",
      "(Iteration 5081 / 24460) loss: 1.051167\n",
      "(Iteration 5101 / 24460) loss: 1.251969\n",
      "(Iteration 5121 / 24460) loss: 1.310049\n",
      "(Iteration 5141 / 24460) loss: 1.117655\n",
      "(Iteration 5161 / 24460) loss: 1.207392\n",
      "(Iteration 5181 / 24460) loss: 0.998166\n",
      "(Iteration 5201 / 24460) loss: 1.250700\n",
      "(Iteration 5221 / 24460) loss: 1.230211\n",
      "(Iteration 5241 / 24460) loss: 1.217338\n",
      "(Iteration 5261 / 24460) loss: 1.062163\n",
      "(Iteration 5281 / 24460) loss: 1.179421\n",
      "(Iteration 5301 / 24460) loss: 1.392089\n",
      "(Iteration 5321 / 24460) loss: 1.324026\n",
      "(Iteration 5341 / 24460) loss: 0.920112\n",
      "(Iteration 5361 / 24460) loss: 1.248448\n",
      "(Iteration 5381 / 24460) loss: 1.112382\n",
      "(Iteration 5401 / 24460) loss: 1.216238\n",
      "(Iteration 5421 / 24460) loss: 0.994350\n",
      "(Iteration 5441 / 24460) loss: 1.352212\n",
      "(Iteration 5461 / 24460) loss: 1.140158\n",
      "(Iteration 5481 / 24460) loss: 1.270309\n",
      "(Iteration 5501 / 24460) loss: 1.065324\n",
      "(Iteration 5521 / 24460) loss: 0.951950\n",
      "(Iteration 5541 / 24460) loss: 1.315495\n",
      "(Iteration 5561 / 24460) loss: 1.201802\n",
      "(Iteration 5581 / 24460) loss: 0.995895\n",
      "(Iteration 5601 / 24460) loss: 1.240260\n",
      "(Iteration 5621 / 24460) loss: 1.317790\n",
      "(Iteration 5641 / 24460) loss: 1.138205\n",
      "(Iteration 5661 / 24460) loss: 1.268956\n",
      "(Iteration 5681 / 24460) loss: 0.997571\n",
      "(Iteration 5701 / 24460) loss: 1.025514\n",
      "(Iteration 5721 / 24460) loss: 0.996115\n",
      "(Iteration 5741 / 24460) loss: 1.142044\n",
      "(Iteration 5761 / 24460) loss: 1.097030\n",
      "(Iteration 5781 / 24460) loss: 0.920819\n",
      "(Iteration 5801 / 24460) loss: 0.829411\n",
      "(Iteration 5821 / 24460) loss: 1.098650\n",
      "(Iteration 5841 / 24460) loss: 0.920301\n",
      "(Iteration 5861 / 24460) loss: 1.099958\n",
      "(Iteration 5881 / 24460) loss: 1.266457\n",
      "(Iteration 5901 / 24460) loss: 0.894161\n",
      "(Iteration 5921 / 24460) loss: 1.065915\n",
      "(Iteration 5941 / 24460) loss: 1.143740\n",
      "(Iteration 5961 / 24460) loss: 1.183432\n",
      "(Iteration 5981 / 24460) loss: 0.963108\n",
      "(Iteration 6001 / 24460) loss: 1.128657\n",
      "(Iteration 6021 / 24460) loss: 1.501144\n",
      "(Iteration 6041 / 24460) loss: 1.243106\n",
      "(Iteration 6061 / 24460) loss: 1.118133\n",
      "(Iteration 6081 / 24460) loss: 1.081439\n",
      "(Iteration 6101 / 24460) loss: 1.099052\n",
      "(Epoch 5 / 20) train acc: 0.663000; val_acc: 0.639000\n",
      "(Iteration 6121 / 24460) loss: 1.129907\n",
      "(Iteration 6141 / 24460) loss: 1.073058\n",
      "(Iteration 6161 / 24460) loss: 1.141271\n",
      "(Iteration 6181 / 24460) loss: 1.084283\n",
      "(Iteration 6201 / 24460) loss: 1.194485\n",
      "(Iteration 6221 / 24460) loss: 0.831768\n",
      "(Iteration 6241 / 24460) loss: 1.106456\n",
      "(Iteration 6261 / 24460) loss: 1.036564\n",
      "(Iteration 6281 / 24460) loss: 1.079901\n",
      "(Iteration 6301 / 24460) loss: 1.223262\n",
      "(Iteration 6321 / 24460) loss: 1.032885\n",
      "(Iteration 6341 / 24460) loss: 1.132415\n",
      "(Iteration 6361 / 24460) loss: 1.098363\n",
      "(Iteration 6381 / 24460) loss: 0.952874\n",
      "(Iteration 6401 / 24460) loss: 0.998185\n",
      "(Iteration 6421 / 24460) loss: 1.003338\n",
      "(Iteration 6441 / 24460) loss: 1.086361\n",
      "(Iteration 6461 / 24460) loss: 1.113846\n",
      "(Iteration 6481 / 24460) loss: 1.139195\n",
      "(Iteration 6501 / 24460) loss: 1.104181\n",
      "(Iteration 6521 / 24460) loss: 1.158977\n",
      "(Iteration 6541 / 24460) loss: 0.857722\n",
      "(Iteration 6561 / 24460) loss: 1.051562\n",
      "(Iteration 6581 / 24460) loss: 1.142748\n",
      "(Iteration 6601 / 24460) loss: 0.953883\n",
      "(Iteration 6621 / 24460) loss: 1.182391\n",
      "(Iteration 6641 / 24460) loss: 1.129789\n",
      "(Iteration 6661 / 24460) loss: 0.962286\n",
      "(Iteration 6681 / 24460) loss: 1.138154\n",
      "(Iteration 6701 / 24460) loss: 0.947399\n",
      "(Iteration 6721 / 24460) loss: 0.938286\n",
      "(Iteration 6741 / 24460) loss: 1.312306\n",
      "(Iteration 6761 / 24460) loss: 1.092289\n",
      "(Iteration 6781 / 24460) loss: 1.110781\n",
      "(Iteration 6801 / 24460) loss: 1.212544\n",
      "(Iteration 6821 / 24460) loss: 1.087380\n",
      "(Iteration 6841 / 24460) loss: 1.088599\n",
      "(Iteration 6861 / 24460) loss: 1.019982\n",
      "(Iteration 6881 / 24460) loss: 1.158196\n",
      "(Iteration 6901 / 24460) loss: 0.909006\n",
      "(Iteration 6921 / 24460) loss: 1.091443\n",
      "(Iteration 6941 / 24460) loss: 0.986882\n",
      "(Iteration 6961 / 24460) loss: 1.210358\n",
      "(Iteration 6981 / 24460) loss: 0.894493\n",
      "(Iteration 7001 / 24460) loss: 1.524403\n",
      "(Iteration 7021 / 24460) loss: 1.267861\n",
      "(Iteration 7041 / 24460) loss: 1.174301\n",
      "(Iteration 7061 / 24460) loss: 1.115855\n",
      "(Iteration 7081 / 24460) loss: 1.108699\n",
      "(Iteration 7101 / 24460) loss: 1.012441\n",
      "(Iteration 7121 / 24460) loss: 1.084963\n",
      "(Iteration 7141 / 24460) loss: 1.062684\n",
      "(Iteration 7161 / 24460) loss: 1.274212\n",
      "(Iteration 7181 / 24460) loss: 1.244564\n",
      "(Iteration 7201 / 24460) loss: 1.095750\n",
      "(Iteration 7221 / 24460) loss: 1.111947\n",
      "(Iteration 7241 / 24460) loss: 1.189074\n",
      "(Iteration 7261 / 24460) loss: 1.111293\n",
      "(Iteration 7281 / 24460) loss: 1.276220\n",
      "(Iteration 7301 / 24460) loss: 1.199976\n",
      "(Iteration 7321 / 24460) loss: 1.154595\n",
      "(Epoch 6 / 20) train acc: 0.679000; val_acc: 0.629000\n",
      "(Iteration 7341 / 24460) loss: 1.028693\n",
      "(Iteration 7361 / 24460) loss: 0.967818\n",
      "(Iteration 7381 / 24460) loss: 1.205283\n",
      "(Iteration 7401 / 24460) loss: 1.153289\n",
      "(Iteration 7421 / 24460) loss: 1.313874\n",
      "(Iteration 7441 / 24460) loss: 1.165045\n",
      "(Iteration 7461 / 24460) loss: 1.017999\n",
      "(Iteration 7481 / 24460) loss: 0.965494\n",
      "(Iteration 7501 / 24460) loss: 0.801104\n",
      "(Iteration 7521 / 24460) loss: 1.250936\n",
      "(Iteration 7541 / 24460) loss: 0.897439\n",
      "(Iteration 7561 / 24460) loss: 0.995711\n",
      "(Iteration 7581 / 24460) loss: 0.979475\n",
      "(Iteration 7601 / 24460) loss: 0.971818\n",
      "(Iteration 7621 / 24460) loss: 1.004061\n",
      "(Iteration 7641 / 24460) loss: 1.206641\n",
      "(Iteration 7661 / 24460) loss: 1.064777\n",
      "(Iteration 7681 / 24460) loss: 0.925063\n",
      "(Iteration 7701 / 24460) loss: 1.063521\n",
      "(Iteration 7721 / 24460) loss: 0.981187\n",
      "(Iteration 7741 / 24460) loss: 1.115322\n",
      "(Iteration 7761 / 24460) loss: 1.191560\n",
      "(Iteration 7781 / 24460) loss: 1.044579\n",
      "(Iteration 7801 / 24460) loss: 1.059569\n",
      "(Iteration 7821 / 24460) loss: 0.856875\n",
      "(Iteration 7841 / 24460) loss: 1.101357\n",
      "(Iteration 7861 / 24460) loss: 1.299703\n",
      "(Iteration 7881 / 24460) loss: 1.143184\n",
      "(Iteration 7901 / 24460) loss: 0.951068\n",
      "(Iteration 7921 / 24460) loss: 1.225451\n",
      "(Iteration 7941 / 24460) loss: 0.890682\n",
      "(Iteration 7961 / 24460) loss: 1.297413\n",
      "(Iteration 7981 / 24460) loss: 1.204189\n",
      "(Iteration 8001 / 24460) loss: 0.982908\n",
      "(Iteration 8021 / 24460) loss: 1.088613\n",
      "(Iteration 8041 / 24460) loss: 1.122282\n",
      "(Iteration 8061 / 24460) loss: 0.943661\n",
      "(Iteration 8081 / 24460) loss: 1.264588\n",
      "(Iteration 8101 / 24460) loss: 1.069648\n",
      "(Iteration 8121 / 24460) loss: 1.119463\n",
      "(Iteration 8141 / 24460) loss: 1.193917\n",
      "(Iteration 8161 / 24460) loss: 0.898159\n",
      "(Iteration 8181 / 24460) loss: 0.866811\n",
      "(Iteration 8201 / 24460) loss: 1.157122\n",
      "(Iteration 8221 / 24460) loss: 0.967785\n",
      "(Iteration 8241 / 24460) loss: 1.159309\n",
      "(Iteration 8261 / 24460) loss: 1.302584\n",
      "(Iteration 8281 / 24460) loss: 0.972762\n",
      "(Iteration 8301 / 24460) loss: 1.085269\n",
      "(Iteration 8321 / 24460) loss: 1.091957\n",
      "(Iteration 8341 / 24460) loss: 1.091986\n",
      "(Iteration 8361 / 24460) loss: 1.188257\n",
      "(Iteration 8381 / 24460) loss: 1.071326\n",
      "(Iteration 8401 / 24460) loss: 1.006811\n",
      "(Iteration 8421 / 24460) loss: 1.190258\n",
      "(Iteration 8441 / 24460) loss: 1.049709\n",
      "(Iteration 8461 / 24460) loss: 0.788960\n",
      "(Iteration 8481 / 24460) loss: 1.051102\n",
      "(Iteration 8501 / 24460) loss: 0.977751\n",
      "(Iteration 8521 / 24460) loss: 1.095597\n",
      "(Iteration 8541 / 24460) loss: 0.973427\n",
      "(Iteration 8561 / 24460) loss: 1.133476\n",
      "(Epoch 7 / 20) train acc: 0.649000; val_acc: 0.642000\n",
      "(Iteration 8581 / 24460) loss: 1.150982\n",
      "(Iteration 8601 / 24460) loss: 1.258263\n",
      "(Iteration 8621 / 24460) loss: 1.138344\n",
      "(Iteration 8641 / 24460) loss: 1.047026\n",
      "(Iteration 8661 / 24460) loss: 1.221310\n",
      "(Iteration 8681 / 24460) loss: 1.067322\n",
      "(Iteration 8701 / 24460) loss: 1.137405\n",
      "(Iteration 8721 / 24460) loss: 1.238991\n",
      "(Iteration 8741 / 24460) loss: 0.963916\n",
      "(Iteration 8761 / 24460) loss: 1.303975\n",
      "(Iteration 8781 / 24460) loss: 1.064782\n",
      "(Iteration 8801 / 24460) loss: 0.818812\n",
      "(Iteration 8821 / 24460) loss: 1.171133\n",
      "(Iteration 8841 / 24460) loss: 0.881442\n",
      "(Iteration 8861 / 24460) loss: 1.081328\n",
      "(Iteration 8881 / 24460) loss: 1.021039\n",
      "(Iteration 8901 / 24460) loss: 0.992392\n",
      "(Iteration 8921 / 24460) loss: 1.159066\n",
      "(Iteration 8941 / 24460) loss: 1.295421\n",
      "(Iteration 8961 / 24460) loss: 1.187107\n",
      "(Iteration 8981 / 24460) loss: 1.095315\n",
      "(Iteration 9001 / 24460) loss: 1.087444\n",
      "(Iteration 9021 / 24460) loss: 1.123139\n",
      "(Iteration 9041 / 24460) loss: 0.999799\n",
      "(Iteration 9061 / 24460) loss: 0.856076\n",
      "(Iteration 9081 / 24460) loss: 0.980105\n",
      "(Iteration 9101 / 24460) loss: 1.239787\n",
      "(Iteration 9121 / 24460) loss: 1.180636\n",
      "(Iteration 9141 / 24460) loss: 1.013291\n",
      "(Iteration 9161 / 24460) loss: 1.057333\n",
      "(Iteration 9181 / 24460) loss: 1.058848\n",
      "(Iteration 9201 / 24460) loss: 0.961219\n",
      "(Iteration 9221 / 24460) loss: 0.925803\n",
      "(Iteration 9241 / 24460) loss: 1.293442\n",
      "(Iteration 9261 / 24460) loss: 1.169147\n",
      "(Iteration 9281 / 24460) loss: 1.154834\n",
      "(Iteration 9301 / 24460) loss: 1.156065\n",
      "(Iteration 9321 / 24460) loss: 0.962587\n",
      "(Iteration 9341 / 24460) loss: 0.927212\n",
      "(Iteration 9361 / 24460) loss: 0.960957\n",
      "(Iteration 9381 / 24460) loss: 0.957741\n",
      "(Iteration 9401 / 24460) loss: 0.938525\n",
      "(Iteration 9421 / 24460) loss: 0.870595\n",
      "(Iteration 9441 / 24460) loss: 0.991194\n",
      "(Iteration 9461 / 24460) loss: 1.087964\n",
      "(Iteration 9481 / 24460) loss: 1.121719\n",
      "(Iteration 9501 / 24460) loss: 0.972605\n",
      "(Iteration 9521 / 24460) loss: 1.045582\n",
      "(Iteration 9541 / 24460) loss: 1.098090\n",
      "(Iteration 9561 / 24460) loss: 1.069079\n",
      "(Iteration 9581 / 24460) loss: 0.969363\n",
      "(Iteration 9601 / 24460) loss: 1.048867\n",
      "(Iteration 9621 / 24460) loss: 0.964241\n",
      "(Iteration 9641 / 24460) loss: 1.177604\n",
      "(Iteration 9661 / 24460) loss: 1.166654\n",
      "(Iteration 9681 / 24460) loss: 0.936888\n",
      "(Iteration 9701 / 24460) loss: 1.103031\n",
      "(Iteration 9721 / 24460) loss: 1.032414\n",
      "(Iteration 9741 / 24460) loss: 1.248121\n",
      "(Iteration 9761 / 24460) loss: 1.333015\n",
      "(Iteration 9781 / 24460) loss: 0.912411\n",
      "(Epoch 8 / 20) train acc: 0.687000; val_acc: 0.638000\n",
      "(Iteration 9801 / 24460) loss: 1.097412\n",
      "(Iteration 9821 / 24460) loss: 0.985956\n",
      "(Iteration 9841 / 24460) loss: 1.028175\n",
      "(Iteration 9861 / 24460) loss: 1.104003\n",
      "(Iteration 9881 / 24460) loss: 1.031309\n",
      "(Iteration 9901 / 24460) loss: 0.939893\n",
      "(Iteration 9921 / 24460) loss: 1.059498\n",
      "(Iteration 9941 / 24460) loss: 1.276190\n",
      "(Iteration 9961 / 24460) loss: 1.183860\n",
      "(Iteration 9981 / 24460) loss: 1.045031\n",
      "(Iteration 10001 / 24460) loss: 1.038027\n",
      "(Iteration 10021 / 24460) loss: 1.024074\n",
      "(Iteration 10041 / 24460) loss: 1.023974\n",
      "(Iteration 10061 / 24460) loss: 1.171422\n",
      "(Iteration 10081 / 24460) loss: 1.057586\n",
      "(Iteration 10101 / 24460) loss: 1.102446\n",
      "(Iteration 10121 / 24460) loss: 1.198798\n",
      "(Iteration 10141 / 24460) loss: 1.129857\n",
      "(Iteration 10161 / 24460) loss: 1.174973\n",
      "(Iteration 10181 / 24460) loss: 1.038794\n",
      "(Iteration 10201 / 24460) loss: 0.967806\n",
      "(Iteration 10221 / 24460) loss: 1.148438\n",
      "(Iteration 10241 / 24460) loss: 1.113279\n",
      "(Iteration 10261 / 24460) loss: 1.135742\n",
      "(Iteration 10281 / 24460) loss: 1.024751\n",
      "(Iteration 10301 / 24460) loss: 0.998011\n",
      "(Iteration 10321 / 24460) loss: 1.018965\n",
      "(Iteration 10341 / 24460) loss: 0.968576\n",
      "(Iteration 10361 / 24460) loss: 1.078993\n",
      "(Iteration 10381 / 24460) loss: 0.996548\n",
      "(Iteration 10401 / 24460) loss: 1.157222\n",
      "(Iteration 10421 / 24460) loss: 1.067188\n",
      "(Iteration 10441 / 24460) loss: 1.020597\n",
      "(Iteration 10461 / 24460) loss: 1.251286\n",
      "(Iteration 10481 / 24460) loss: 1.223498\n",
      "(Iteration 10501 / 24460) loss: 1.138038\n",
      "(Iteration 10521 / 24460) loss: 1.183617\n",
      "(Iteration 10541 / 24460) loss: 0.958307\n",
      "(Iteration 10561 / 24460) loss: 1.253840\n",
      "(Iteration 10581 / 24460) loss: 0.989114\n",
      "(Iteration 10601 / 24460) loss: 1.278203\n",
      "(Iteration 10621 / 24460) loss: 1.031079\n",
      "(Iteration 10641 / 24460) loss: 1.096198\n",
      "(Iteration 10661 / 24460) loss: 1.052968\n",
      "(Iteration 10681 / 24460) loss: 1.117115\n",
      "(Iteration 10701 / 24460) loss: 1.200436\n",
      "(Iteration 10721 / 24460) loss: 1.249390\n",
      "(Iteration 10741 / 24460) loss: 1.130456\n",
      "(Iteration 10761 / 24460) loss: 0.944164\n",
      "(Iteration 10781 / 24460) loss: 1.056075\n",
      "(Iteration 10801 / 24460) loss: 1.099870\n",
      "(Iteration 10821 / 24460) loss: 1.233391\n",
      "(Iteration 10841 / 24460) loss: 1.011289\n",
      "(Iteration 10861 / 24460) loss: 0.892174\n",
      "(Iteration 10881 / 24460) loss: 0.870685\n",
      "(Iteration 10901 / 24460) loss: 1.046534\n",
      "(Iteration 10921 / 24460) loss: 1.216591\n",
      "(Iteration 10941 / 24460) loss: 0.972797\n",
      "(Iteration 10961 / 24460) loss: 0.991299\n",
      "(Iteration 10981 / 24460) loss: 1.052651\n",
      "(Iteration 11001 / 24460) loss: 0.981184\n",
      "(Epoch 9 / 20) train acc: 0.678000; val_acc: 0.636000\n",
      "(Iteration 11021 / 24460) loss: 1.066156\n",
      "(Iteration 11041 / 24460) loss: 1.188949\n",
      "(Iteration 11061 / 24460) loss: 1.132900\n",
      "(Iteration 11081 / 24460) loss: 1.087312\n",
      "(Iteration 11101 / 24460) loss: 1.003873\n",
      "(Iteration 11121 / 24460) loss: 1.026794\n",
      "(Iteration 11141 / 24460) loss: 0.964611\n",
      "(Iteration 11161 / 24460) loss: 1.047817\n",
      "(Iteration 11181 / 24460) loss: 1.017285\n",
      "(Iteration 11201 / 24460) loss: 0.943714\n",
      "(Iteration 11221 / 24460) loss: 1.036221\n",
      "(Iteration 11241 / 24460) loss: 0.974081\n",
      "(Iteration 11261 / 24460) loss: 0.804067\n",
      "(Iteration 11281 / 24460) loss: 0.898356\n",
      "(Iteration 11301 / 24460) loss: 0.977749\n",
      "(Iteration 11321 / 24460) loss: 1.059252\n",
      "(Iteration 11341 / 24460) loss: 0.953022\n",
      "(Iteration 11361 / 24460) loss: 1.086798\n",
      "(Iteration 11381 / 24460) loss: 1.104006\n",
      "(Iteration 11401 / 24460) loss: 1.050733\n",
      "(Iteration 11421 / 24460) loss: 1.119143\n",
      "(Iteration 11441 / 24460) loss: 0.907106\n",
      "(Iteration 11461 / 24460) loss: 1.200150\n",
      "(Iteration 11481 / 24460) loss: 1.211437\n",
      "(Iteration 11501 / 24460) loss: 0.754366\n",
      "(Iteration 11521 / 24460) loss: 0.991990\n",
      "(Iteration 11541 / 24460) loss: 0.978120\n",
      "(Iteration 11561 / 24460) loss: 0.968699\n",
      "(Iteration 11581 / 24460) loss: 1.051384\n",
      "(Iteration 11601 / 24460) loss: 1.001912\n",
      "(Iteration 11621 / 24460) loss: 1.070563\n",
      "(Iteration 11641 / 24460) loss: 0.896923\n",
      "(Iteration 11661 / 24460) loss: 0.887776\n",
      "(Iteration 11681 / 24460) loss: 1.282454\n",
      "(Iteration 11701 / 24460) loss: 1.294549\n",
      "(Iteration 11721 / 24460) loss: 1.202442\n",
      "(Iteration 11741 / 24460) loss: 1.036215\n",
      "(Iteration 11761 / 24460) loss: 1.004647\n",
      "(Iteration 11781 / 24460) loss: 1.047714\n",
      "(Iteration 11801 / 24460) loss: 0.826549\n",
      "(Iteration 11821 / 24460) loss: 0.992534\n",
      "(Iteration 11841 / 24460) loss: 1.088050\n",
      "(Iteration 11861 / 24460) loss: 1.044917\n",
      "(Iteration 11881 / 24460) loss: 0.987356\n",
      "(Iteration 11901 / 24460) loss: 0.976187\n",
      "(Iteration 11921 / 24460) loss: 1.224390\n",
      "(Iteration 11941 / 24460) loss: 1.115215\n",
      "(Iteration 11961 / 24460) loss: 1.037788\n",
      "(Iteration 11981 / 24460) loss: 1.080773\n",
      "(Iteration 12001 / 24460) loss: 1.164847\n",
      "(Iteration 12021 / 24460) loss: 0.881558\n",
      "(Iteration 12041 / 24460) loss: 0.903563\n",
      "(Iteration 12061 / 24460) loss: 1.149447\n",
      "(Iteration 12081 / 24460) loss: 1.112572\n",
      "(Iteration 12101 / 24460) loss: 0.765414\n",
      "(Iteration 12121 / 24460) loss: 1.056328\n",
      "(Iteration 12141 / 24460) loss: 1.036165\n",
      "(Iteration 12161 / 24460) loss: 0.964383\n",
      "(Iteration 12181 / 24460) loss: 1.130474\n",
      "(Iteration 12201 / 24460) loss: 0.966891\n",
      "(Iteration 12221 / 24460) loss: 0.860122\n",
      "(Epoch 10 / 20) train acc: 0.713000; val_acc: 0.656000\n",
      "(Iteration 12241 / 24460) loss: 1.052770\n",
      "(Iteration 12261 / 24460) loss: 0.970801\n",
      "(Iteration 12281 / 24460) loss: 1.192567\n",
      "(Iteration 12301 / 24460) loss: 1.134865\n",
      "(Iteration 12321 / 24460) loss: 1.102948\n",
      "(Iteration 12341 / 24460) loss: 0.923098\n",
      "(Iteration 12361 / 24460) loss: 1.197572\n",
      "(Iteration 12381 / 24460) loss: 1.135824\n",
      "(Iteration 12401 / 24460) loss: 1.090383\n",
      "(Iteration 12421 / 24460) loss: 1.107946\n",
      "(Iteration 12441 / 24460) loss: 0.999766\n",
      "(Iteration 12461 / 24460) loss: 1.171789\n",
      "(Iteration 12481 / 24460) loss: 0.831641\n",
      "(Iteration 12501 / 24460) loss: 1.035091\n",
      "(Iteration 12521 / 24460) loss: 1.118523\n",
      "(Iteration 12541 / 24460) loss: 1.117638\n",
      "(Iteration 12561 / 24460) loss: 1.085577\n",
      "(Iteration 12581 / 24460) loss: 1.377333\n",
      "(Iteration 12601 / 24460) loss: 1.230613\n",
      "(Iteration 12621 / 24460) loss: 1.076819\n",
      "(Iteration 12641 / 24460) loss: 1.154344\n",
      "(Iteration 12661 / 24460) loss: 1.133345\n",
      "(Iteration 12681 / 24460) loss: 1.100039\n",
      "(Iteration 12701 / 24460) loss: 1.063743\n",
      "(Iteration 12721 / 24460) loss: 1.059160\n",
      "(Iteration 12741 / 24460) loss: 1.087043\n",
      "(Iteration 12761 / 24460) loss: 1.188213\n",
      "(Iteration 12781 / 24460) loss: 0.947083\n",
      "(Iteration 12801 / 24460) loss: 0.874229\n",
      "(Iteration 12821 / 24460) loss: 1.014170\n",
      "(Iteration 12841 / 24460) loss: 1.121244\n",
      "(Iteration 12861 / 24460) loss: 1.063940\n",
      "(Iteration 12881 / 24460) loss: 1.253822\n",
      "(Iteration 12901 / 24460) loss: 1.055508\n",
      "(Iteration 12921 / 24460) loss: 0.934860\n",
      "(Iteration 12941 / 24460) loss: 1.144986\n",
      "(Iteration 12961 / 24460) loss: 1.000696\n",
      "(Iteration 12981 / 24460) loss: 0.997892\n",
      "(Iteration 13001 / 24460) loss: 0.841473\n",
      "(Iteration 13021 / 24460) loss: 1.095416\n",
      "(Iteration 13041 / 24460) loss: 0.959957\n",
      "(Iteration 13061 / 24460) loss: 0.961732\n",
      "(Iteration 13081 / 24460) loss: 0.921939\n",
      "(Iteration 13101 / 24460) loss: 0.987105\n",
      "(Iteration 13121 / 24460) loss: 0.861549\n",
      "(Iteration 13141 / 24460) loss: 1.016953\n",
      "(Iteration 13161 / 24460) loss: 0.951785\n",
      "(Iteration 13181 / 24460) loss: 1.215004\n",
      "(Iteration 13201 / 24460) loss: 0.849733\n",
      "(Iteration 13221 / 24460) loss: 0.910898\n",
      "(Iteration 13241 / 24460) loss: 0.997898\n",
      "(Iteration 13261 / 24460) loss: 0.916990\n",
      "(Iteration 13281 / 24460) loss: 0.996246\n",
      "(Iteration 13301 / 24460) loss: 1.134790\n",
      "(Iteration 13321 / 24460) loss: 0.993735\n",
      "(Iteration 13341 / 24460) loss: 1.095465\n",
      "(Iteration 13361 / 24460) loss: 1.073289\n",
      "(Iteration 13381 / 24460) loss: 1.264105\n",
      "(Iteration 13401 / 24460) loss: 0.839522\n",
      "(Iteration 13421 / 24460) loss: 1.034215\n",
      "(Iteration 13441 / 24460) loss: 0.916255\n",
      "(Epoch 11 / 20) train acc: 0.675000; val_acc: 0.653000\n",
      "(Iteration 13461 / 24460) loss: 0.871069\n",
      "(Iteration 13481 / 24460) loss: 1.073814\n",
      "(Iteration 13501 / 24460) loss: 1.057457\n",
      "(Iteration 13521 / 24460) loss: 1.045322\n",
      "(Iteration 13541 / 24460) loss: 1.129634\n",
      "(Iteration 13561 / 24460) loss: 0.868889\n",
      "(Iteration 13581 / 24460) loss: 1.064611\n",
      "(Iteration 13601 / 24460) loss: 1.016197\n",
      "(Iteration 13621 / 24460) loss: 1.267896\n",
      "(Iteration 13641 / 24460) loss: 0.863097\n",
      "(Iteration 13661 / 24460) loss: 0.966914\n",
      "(Iteration 13681 / 24460) loss: 1.016997\n",
      "(Iteration 13701 / 24460) loss: 1.153994\n",
      "(Iteration 13721 / 24460) loss: 0.999021\n",
      "(Iteration 13741 / 24460) loss: 1.083256\n",
      "(Iteration 13761 / 24460) loss: 0.951712\n",
      "(Iteration 13781 / 24460) loss: 1.209516\n",
      "(Iteration 13801 / 24460) loss: 1.083192\n",
      "(Iteration 13821 / 24460) loss: 1.016923\n",
      "(Iteration 13841 / 24460) loss: 0.916959\n",
      "(Iteration 13861 / 24460) loss: 1.065910\n",
      "(Iteration 13881 / 24460) loss: 1.206309\n",
      "(Iteration 13901 / 24460) loss: 1.080893\n",
      "(Iteration 13921 / 24460) loss: 1.043711\n",
      "(Iteration 13941 / 24460) loss: 0.982596\n",
      "(Iteration 13961 / 24460) loss: 0.990555\n",
      "(Iteration 13981 / 24460) loss: 1.051925\n",
      "(Iteration 14001 / 24460) loss: 1.027502\n",
      "(Iteration 14021 / 24460) loss: 1.207919\n",
      "(Iteration 14041 / 24460) loss: 1.084263\n",
      "(Iteration 14061 / 24460) loss: 1.169087\n",
      "(Iteration 14081 / 24460) loss: 0.903914\n",
      "(Iteration 14101 / 24460) loss: 0.914025\n",
      "(Iteration 14121 / 24460) loss: 0.998631\n",
      "(Iteration 14141 / 24460) loss: 1.200306\n",
      "(Iteration 14161 / 24460) loss: 1.104157\n",
      "(Iteration 14181 / 24460) loss: 0.924816\n",
      "(Iteration 14201 / 24460) loss: 1.292522\n",
      "(Iteration 14221 / 24460) loss: 1.028224\n",
      "(Iteration 14241 / 24460) loss: 0.968215\n",
      "(Iteration 14261 / 24460) loss: 1.000287\n",
      "(Iteration 14281 / 24460) loss: 1.141932\n",
      "(Iteration 14301 / 24460) loss: 1.084227\n",
      "(Iteration 14321 / 24460) loss: 0.952204\n",
      "(Iteration 14341 / 24460) loss: 1.030548\n",
      "(Iteration 14361 / 24460) loss: 1.031550\n",
      "(Iteration 14381 / 24460) loss: 0.996652\n",
      "(Iteration 14401 / 24460) loss: 1.282737\n",
      "(Iteration 14421 / 24460) loss: 0.996048\n",
      "(Iteration 14441 / 24460) loss: 1.044553\n",
      "(Iteration 14461 / 24460) loss: 0.840014\n",
      "(Iteration 14481 / 24460) loss: 1.122835\n",
      "(Iteration 14501 / 24460) loss: 1.082399\n",
      "(Iteration 14521 / 24460) loss: 1.061275\n",
      "(Iteration 14541 / 24460) loss: 0.966500\n",
      "(Iteration 14561 / 24460) loss: 1.036658\n",
      "(Iteration 14581 / 24460) loss: 1.038086\n",
      "(Iteration 14601 / 24460) loss: 1.014901\n",
      "(Iteration 14621 / 24460) loss: 1.002290\n",
      "(Iteration 14641 / 24460) loss: 0.901779\n",
      "(Iteration 14661 / 24460) loss: 1.005527\n",
      "(Epoch 12 / 20) train acc: 0.678000; val_acc: 0.663000\n",
      "(Iteration 14681 / 24460) loss: 0.881486\n",
      "(Iteration 14701 / 24460) loss: 1.287045\n",
      "(Iteration 14721 / 24460) loss: 0.923514\n",
      "(Iteration 14741 / 24460) loss: 1.108627\n",
      "(Iteration 14761 / 24460) loss: 0.968628\n",
      "(Iteration 14781 / 24460) loss: 1.192262\n",
      "(Iteration 14801 / 24460) loss: 1.116888\n",
      "(Iteration 14821 / 24460) loss: 1.073804\n",
      "(Iteration 14841 / 24460) loss: 1.087936\n",
      "(Iteration 14861 / 24460) loss: 1.025539\n",
      "(Iteration 14881 / 24460) loss: 0.926272\n",
      "(Iteration 14901 / 24460) loss: 1.123477\n",
      "(Iteration 14921 / 24460) loss: 1.114028\n",
      "(Iteration 14941 / 24460) loss: 0.929525\n",
      "(Iteration 14961 / 24460) loss: 0.875629\n",
      "(Iteration 14981 / 24460) loss: 0.892974\n",
      "(Iteration 15001 / 24460) loss: 0.931920\n",
      "(Iteration 15021 / 24460) loss: 1.230533\n",
      "(Iteration 15041 / 24460) loss: 0.861700\n",
      "(Iteration 15061 / 24460) loss: 1.206768\n",
      "(Iteration 15081 / 24460) loss: 1.024184\n",
      "(Iteration 15101 / 24460) loss: 1.204219\n",
      "(Iteration 15121 / 24460) loss: 1.081250\n",
      "(Iteration 15141 / 24460) loss: 1.242850\n",
      "(Iteration 15161 / 24460) loss: 0.971097\n",
      "(Iteration 15181 / 24460) loss: 1.046391\n",
      "(Iteration 15201 / 24460) loss: 0.888377\n",
      "(Iteration 15221 / 24460) loss: 1.098030\n",
      "(Iteration 15241 / 24460) loss: 1.132702\n",
      "(Iteration 15261 / 24460) loss: 1.199652\n",
      "(Iteration 15281 / 24460) loss: 1.372494\n",
      "(Iteration 15301 / 24460) loss: 1.136523\n",
      "(Iteration 15321 / 24460) loss: 1.027049\n",
      "(Iteration 15341 / 24460) loss: 1.031172\n",
      "(Iteration 15361 / 24460) loss: 1.032550\n",
      "(Iteration 15381 / 24460) loss: 0.963458\n",
      "(Iteration 15401 / 24460) loss: 0.905759\n",
      "(Iteration 15421 / 24460) loss: 1.043518\n",
      "(Iteration 15441 / 24460) loss: 1.114398\n",
      "(Iteration 15461 / 24460) loss: 1.114779\n",
      "(Iteration 15481 / 24460) loss: 0.912827\n",
      "(Iteration 15501 / 24460) loss: 1.059740\n",
      "(Iteration 15521 / 24460) loss: 0.867980\n",
      "(Iteration 15541 / 24460) loss: 1.112523\n",
      "(Iteration 15561 / 24460) loss: 1.060135\n",
      "(Iteration 15581 / 24460) loss: 1.059116\n",
      "(Iteration 15601 / 24460) loss: 1.129436\n",
      "(Iteration 15621 / 24460) loss: 0.994034\n",
      "(Iteration 15641 / 24460) loss: 1.120166\n",
      "(Iteration 15661 / 24460) loss: 1.228634\n",
      "(Iteration 15681 / 24460) loss: 0.998708\n",
      "(Iteration 15701 / 24460) loss: 0.917941\n",
      "(Iteration 15721 / 24460) loss: 0.964858\n",
      "(Iteration 15741 / 24460) loss: 1.027250\n",
      "(Iteration 15761 / 24460) loss: 1.111651\n",
      "(Iteration 15781 / 24460) loss: 0.807349\n",
      "(Iteration 15801 / 24460) loss: 0.939413\n",
      "(Iteration 15821 / 24460) loss: 1.083140\n",
      "(Iteration 15841 / 24460) loss: 1.024578\n",
      "(Iteration 15861 / 24460) loss: 1.069784\n",
      "(Iteration 15881 / 24460) loss: 1.195064\n",
      "(Epoch 13 / 20) train acc: 0.695000; val_acc: 0.660000\n",
      "(Iteration 15901 / 24460) loss: 0.916705\n",
      "(Iteration 15921 / 24460) loss: 0.915117\n",
      "(Iteration 15941 / 24460) loss: 1.171014\n",
      "(Iteration 15961 / 24460) loss: 1.074659\n",
      "(Iteration 15981 / 24460) loss: 1.004918\n",
      "(Iteration 16001 / 24460) loss: 0.950459\n",
      "(Iteration 16021 / 24460) loss: 0.943545\n",
      "(Iteration 16041 / 24460) loss: 0.911328\n",
      "(Iteration 16061 / 24460) loss: 1.155470\n",
      "(Iteration 16081 / 24460) loss: 1.055234\n",
      "(Iteration 16101 / 24460) loss: 0.878317\n",
      "(Iteration 16121 / 24460) loss: 0.959692\n",
      "(Iteration 16141 / 24460) loss: 1.121924\n",
      "(Iteration 16161 / 24460) loss: 0.928708\n",
      "(Iteration 16181 / 24460) loss: 1.149211\n",
      "(Iteration 16201 / 24460) loss: 1.051510\n",
      "(Iteration 16221 / 24460) loss: 0.997013\n",
      "(Iteration 16241 / 24460) loss: 1.009884\n",
      "(Iteration 16261 / 24460) loss: 0.836328\n",
      "(Iteration 16281 / 24460) loss: 0.925528\n",
      "(Iteration 16301 / 24460) loss: 1.017415\n",
      "(Iteration 16321 / 24460) loss: 1.120647\n",
      "(Iteration 16341 / 24460) loss: 1.213527\n",
      "(Iteration 16361 / 24460) loss: 1.047769\n",
      "(Iteration 16381 / 24460) loss: 0.788155\n",
      "(Iteration 16401 / 24460) loss: 1.067019\n",
      "(Iteration 16421 / 24460) loss: 1.094355\n",
      "(Iteration 16441 / 24460) loss: 0.910270\n",
      "(Iteration 16461 / 24460) loss: 1.004971\n",
      "(Iteration 16481 / 24460) loss: 0.889767\n",
      "(Iteration 16501 / 24460) loss: 0.986653\n",
      "(Iteration 16521 / 24460) loss: 1.027288\n",
      "(Iteration 16541 / 24460) loss: 0.673533\n",
      "(Iteration 16561 / 24460) loss: 0.949765\n",
      "(Iteration 16581 / 24460) loss: 1.099160\n",
      "(Iteration 16601 / 24460) loss: 1.065408\n",
      "(Iteration 16621 / 24460) loss: 1.278286\n",
      "(Iteration 16641 / 24460) loss: 1.040013\n",
      "(Iteration 16661 / 24460) loss: 0.887716\n",
      "(Iteration 16681 / 24460) loss: 0.836183\n",
      "(Iteration 16701 / 24460) loss: 1.001875\n",
      "(Iteration 16721 / 24460) loss: 1.113995\n",
      "(Iteration 16741 / 24460) loss: 0.858828\n",
      "(Iteration 16761 / 24460) loss: 0.966579\n",
      "(Iteration 16781 / 24460) loss: 0.895353\n",
      "(Iteration 16801 / 24460) loss: 0.954584\n",
      "(Iteration 16821 / 24460) loss: 1.030165\n",
      "(Iteration 16841 / 24460) loss: 0.919301\n",
      "(Iteration 16861 / 24460) loss: 0.854855\n",
      "(Iteration 16881 / 24460) loss: 0.869481\n",
      "(Iteration 16901 / 24460) loss: 1.120940\n",
      "(Iteration 16921 / 24460) loss: 1.011558\n",
      "(Iteration 16941 / 24460) loss: 0.911969\n",
      "(Iteration 16961 / 24460) loss: 1.012530\n",
      "(Iteration 16981 / 24460) loss: 0.890771\n",
      "(Iteration 17001 / 24460) loss: 1.054197\n",
      "(Iteration 17021 / 24460) loss: 0.801266\n",
      "(Iteration 17041 / 24460) loss: 0.803925\n",
      "(Iteration 17061 / 24460) loss: 1.128880\n",
      "(Iteration 17081 / 24460) loss: 0.808644\n",
      "(Iteration 17101 / 24460) loss: 1.176471\n",
      "(Iteration 17121 / 24460) loss: 0.939181\n",
      "(Epoch 14 / 20) train acc: 0.685000; val_acc: 0.654000\n",
      "(Iteration 17141 / 24460) loss: 0.994614\n",
      "(Iteration 17161 / 24460) loss: 0.882954\n",
      "(Iteration 17181 / 24460) loss: 0.851716\n",
      "(Iteration 17201 / 24460) loss: 1.254913\n",
      "(Iteration 17221 / 24460) loss: 0.937979\n",
      "(Iteration 17241 / 24460) loss: 0.895342\n",
      "(Iteration 17261 / 24460) loss: 1.032736\n",
      "(Iteration 17281 / 24460) loss: 1.086330\n",
      "(Iteration 17301 / 24460) loss: 1.171603\n",
      "(Iteration 17321 / 24460) loss: 0.798791\n",
      "(Iteration 17341 / 24460) loss: 1.064891\n",
      "(Iteration 17361 / 24460) loss: 0.967917\n",
      "(Iteration 17381 / 24460) loss: 1.028399\n",
      "(Iteration 17401 / 24460) loss: 1.111676\n",
      "(Iteration 17421 / 24460) loss: 1.041382\n",
      "(Iteration 17441 / 24460) loss: 0.941041\n",
      "(Iteration 17461 / 24460) loss: 0.831273\n",
      "(Iteration 17481 / 24460) loss: 0.979104\n",
      "(Iteration 17501 / 24460) loss: 0.798523\n",
      "(Iteration 17521 / 24460) loss: 0.968189\n",
      "(Iteration 17541 / 24460) loss: 0.855786\n",
      "(Iteration 17561 / 24460) loss: 0.974229\n",
      "(Iteration 17581 / 24460) loss: 0.936157\n",
      "(Iteration 17601 / 24460) loss: 1.053959\n",
      "(Iteration 17621 / 24460) loss: 1.102445\n",
      "(Iteration 17641 / 24460) loss: 1.121313\n",
      "(Iteration 17661 / 24460) loss: 0.924026\n",
      "(Iteration 17681 / 24460) loss: 1.177346\n",
      "(Iteration 17701 / 24460) loss: 0.884149\n",
      "(Iteration 17721 / 24460) loss: 1.071664\n",
      "(Iteration 17741 / 24460) loss: 0.970736\n",
      "(Iteration 17761 / 24460) loss: 1.042843\n",
      "(Iteration 17781 / 24460) loss: 1.125770\n",
      "(Iteration 17801 / 24460) loss: 1.247933\n",
      "(Iteration 17821 / 24460) loss: 0.830458\n",
      "(Iteration 17841 / 24460) loss: 1.056150\n",
      "(Iteration 17861 / 24460) loss: 0.971811\n",
      "(Iteration 17881 / 24460) loss: 0.825947\n",
      "(Iteration 17901 / 24460) loss: 1.136552\n",
      "(Iteration 17921 / 24460) loss: 0.953373\n",
      "(Iteration 17941 / 24460) loss: 0.901938\n",
      "(Iteration 17961 / 24460) loss: 0.916282\n",
      "(Iteration 17981 / 24460) loss: 0.956023\n",
      "(Iteration 18001 / 24460) loss: 0.859028\n",
      "(Iteration 18021 / 24460) loss: 1.110694\n",
      "(Iteration 18041 / 24460) loss: 1.050298\n",
      "(Iteration 18061 / 24460) loss: 0.941958\n",
      "(Iteration 18081 / 24460) loss: 1.004586\n",
      "(Iteration 18101 / 24460) loss: 1.210491\n",
      "(Iteration 18121 / 24460) loss: 0.869880\n",
      "(Iteration 18141 / 24460) loss: 1.017760\n",
      "(Iteration 18161 / 24460) loss: 0.975835\n",
      "(Iteration 18181 / 24460) loss: 0.850423\n",
      "(Iteration 18201 / 24460) loss: 0.981225\n",
      "(Iteration 18221 / 24460) loss: 0.849840\n",
      "(Iteration 18241 / 24460) loss: 0.989565\n",
      "(Iteration 18261 / 24460) loss: 1.059574\n",
      "(Iteration 18281 / 24460) loss: 1.090118\n",
      "(Iteration 18301 / 24460) loss: 0.848001\n",
      "(Iteration 18321 / 24460) loss: 0.977244\n",
      "(Iteration 18341 / 24460) loss: 1.021586\n",
      "(Epoch 15 / 20) train acc: 0.705000; val_acc: 0.659000\n",
      "(Iteration 18361 / 24460) loss: 1.267448\n",
      "(Iteration 18381 / 24460) loss: 0.877522\n",
      "(Iteration 18401 / 24460) loss: 0.987045\n",
      "(Iteration 18421 / 24460) loss: 0.920265\n",
      "(Iteration 18441 / 24460) loss: 1.018046\n",
      "(Iteration 18461 / 24460) loss: 1.119168\n",
      "(Iteration 18481 / 24460) loss: 1.035933\n",
      "(Iteration 18501 / 24460) loss: 1.230830\n",
      "(Iteration 18521 / 24460) loss: 0.932665\n",
      "(Iteration 18541 / 24460) loss: 1.100426\n",
      "(Iteration 18561 / 24460) loss: 1.024486\n",
      "(Iteration 18581 / 24460) loss: 1.207343\n",
      "(Iteration 18601 / 24460) loss: 0.758447\n",
      "(Iteration 18621 / 24460) loss: 1.066342\n",
      "(Iteration 18641 / 24460) loss: 1.058629\n",
      "(Iteration 18661 / 24460) loss: 0.918161\n",
      "(Iteration 18681 / 24460) loss: 1.133171\n",
      "(Iteration 18701 / 24460) loss: 0.927118\n",
      "(Iteration 18721 / 24460) loss: 1.023816\n",
      "(Iteration 18741 / 24460) loss: 1.028747\n",
      "(Iteration 18761 / 24460) loss: 0.879278\n",
      "(Iteration 18781 / 24460) loss: 0.983418\n",
      "(Iteration 18801 / 24460) loss: 0.799960\n",
      "(Iteration 18821 / 24460) loss: 1.112307\n",
      "(Iteration 18841 / 24460) loss: 1.040959\n",
      "(Iteration 18861 / 24460) loss: 1.104302\n",
      "(Iteration 18881 / 24460) loss: 1.092357\n",
      "(Iteration 18901 / 24460) loss: 1.294326\n",
      "(Iteration 18921 / 24460) loss: 0.702518\n",
      "(Iteration 18941 / 24460) loss: 1.150760\n",
      "(Iteration 18961 / 24460) loss: 1.071607\n",
      "(Iteration 18981 / 24460) loss: 0.844091\n",
      "(Iteration 19001 / 24460) loss: 1.074861\n",
      "(Iteration 19021 / 24460) loss: 1.209561\n",
      "(Iteration 19041 / 24460) loss: 1.071704\n",
      "(Iteration 19061 / 24460) loss: 1.059813\n",
      "(Iteration 19081 / 24460) loss: 0.942126\n",
      "(Iteration 19101 / 24460) loss: 0.940936\n",
      "(Iteration 19121 / 24460) loss: 1.121654\n",
      "(Iteration 19141 / 24460) loss: 0.967311\n",
      "(Iteration 19161 / 24460) loss: 1.058338\n",
      "(Iteration 19181 / 24460) loss: 1.104062\n",
      "(Iteration 19201 / 24460) loss: 1.024871\n",
      "(Iteration 19221 / 24460) loss: 1.086265\n",
      "(Iteration 19241 / 24460) loss: 1.230465\n",
      "(Iteration 19261 / 24460) loss: 0.773201\n",
      "(Iteration 19281 / 24460) loss: 0.941570\n",
      "(Iteration 19301 / 24460) loss: 0.818016\n",
      "(Iteration 19321 / 24460) loss: 1.115881\n",
      "(Iteration 19341 / 24460) loss: 0.877132\n",
      "(Iteration 19361 / 24460) loss: 1.086820\n",
      "(Iteration 19381 / 24460) loss: 0.995085\n",
      "(Iteration 19401 / 24460) loss: 1.138334\n",
      "(Iteration 19421 / 24460) loss: 0.798456\n",
      "(Iteration 19441 / 24460) loss: 1.066905\n",
      "(Iteration 19461 / 24460) loss: 0.917590\n",
      "(Iteration 19481 / 24460) loss: 0.821619\n",
      "(Iteration 19501 / 24460) loss: 0.847718\n",
      "(Iteration 19521 / 24460) loss: 0.818222\n",
      "(Iteration 19541 / 24460) loss: 0.941492\n",
      "(Iteration 19561 / 24460) loss: 1.150525\n",
      "(Epoch 16 / 20) train acc: 0.738000; val_acc: 0.663000\n",
      "(Iteration 19581 / 24460) loss: 0.933560\n",
      "(Iteration 19601 / 24460) loss: 0.932974\n",
      "(Iteration 19621 / 24460) loss: 0.960941\n",
      "(Iteration 19641 / 24460) loss: 1.010069\n",
      "(Iteration 19661 / 24460) loss: 0.937096\n",
      "(Iteration 19681 / 24460) loss: 1.108464\n",
      "(Iteration 19701 / 24460) loss: 1.126899\n",
      "(Iteration 19721 / 24460) loss: 1.153049\n",
      "(Iteration 19741 / 24460) loss: 0.927695\n",
      "(Iteration 19761 / 24460) loss: 1.140857\n",
      "(Iteration 19781 / 24460) loss: 0.934053\n",
      "(Iteration 19801 / 24460) loss: 0.924502\n",
      "(Iteration 19821 / 24460) loss: 0.986542\n",
      "(Iteration 19841 / 24460) loss: 1.110408\n",
      "(Iteration 19861 / 24460) loss: 1.109872\n",
      "(Iteration 19881 / 24460) loss: 0.820025\n",
      "(Iteration 19901 / 24460) loss: 0.939441\n",
      "(Iteration 19921 / 24460) loss: 1.096329\n",
      "(Iteration 19941 / 24460) loss: 0.772496\n",
      "(Iteration 19961 / 24460) loss: 0.783721\n",
      "(Iteration 19981 / 24460) loss: 0.740935\n",
      "(Iteration 20001 / 24460) loss: 0.850185\n",
      "(Iteration 20021 / 24460) loss: 0.967168\n",
      "(Iteration 20041 / 24460) loss: 1.026493\n",
      "(Iteration 20061 / 24460) loss: 1.076235\n",
      "(Iteration 20081 / 24460) loss: 0.683056\n",
      "(Iteration 20101 / 24460) loss: 1.126611\n",
      "(Iteration 20121 / 24460) loss: 1.043414\n",
      "(Iteration 20141 / 24460) loss: 0.883252\n",
      "(Iteration 20161 / 24460) loss: 1.119689\n",
      "(Iteration 20181 / 24460) loss: 1.014996\n",
      "(Iteration 20201 / 24460) loss: 0.917797\n",
      "(Iteration 20221 / 24460) loss: 1.060991\n",
      "(Iteration 20241 / 24460) loss: 1.023356\n",
      "(Iteration 20261 / 24460) loss: 1.140276\n",
      "(Iteration 20281 / 24460) loss: 1.073636\n",
      "(Iteration 20301 / 24460) loss: 0.941061\n",
      "(Iteration 20321 / 24460) loss: 1.152275\n",
      "(Iteration 20341 / 24460) loss: 0.926957\n",
      "(Iteration 20361 / 24460) loss: 1.088396\n",
      "(Iteration 20381 / 24460) loss: 1.150701\n",
      "(Iteration 20401 / 24460) loss: 0.877152\n",
      "(Iteration 20421 / 24460) loss: 1.014255\n",
      "(Iteration 20441 / 24460) loss: 1.087001\n",
      "(Iteration 20461 / 24460) loss: 1.038355\n",
      "(Iteration 20481 / 24460) loss: 0.827384\n",
      "(Iteration 20501 / 24460) loss: 0.995229\n",
      "(Iteration 20521 / 24460) loss: 1.035703\n",
      "(Iteration 20541 / 24460) loss: 1.074615\n",
      "(Iteration 20561 / 24460) loss: 1.138568\n",
      "(Iteration 20581 / 24460) loss: 0.933145\n",
      "(Iteration 20601 / 24460) loss: 1.007828\n",
      "(Iteration 20621 / 24460) loss: 1.041655\n",
      "(Iteration 20641 / 24460) loss: 1.016891\n",
      "(Iteration 20661 / 24460) loss: 0.746718\n",
      "(Iteration 20681 / 24460) loss: 1.116830\n",
      "(Iteration 20701 / 24460) loss: 1.170004\n",
      "(Iteration 20721 / 24460) loss: 1.099780\n",
      "(Iteration 20741 / 24460) loss: 1.028325\n",
      "(Iteration 20761 / 24460) loss: 0.834976\n",
      "(Iteration 20781 / 24460) loss: 1.022568\n",
      "(Epoch 17 / 20) train acc: 0.731000; val_acc: 0.640000\n",
      "(Iteration 20801 / 24460) loss: 0.922046\n",
      "(Iteration 20821 / 24460) loss: 1.174673\n",
      "(Iteration 20841 / 24460) loss: 0.950158\n",
      "(Iteration 20861 / 24460) loss: 0.768901\n",
      "(Iteration 20881 / 24460) loss: 0.959578\n",
      "(Iteration 20901 / 24460) loss: 1.329108\n",
      "(Iteration 20921 / 24460) loss: 1.082409\n",
      "(Iteration 20941 / 24460) loss: 1.202541\n",
      "(Iteration 20961 / 24460) loss: 1.064210\n",
      "(Iteration 20981 / 24460) loss: 1.060530\n",
      "(Iteration 21001 / 24460) loss: 0.934925\n",
      "(Iteration 21021 / 24460) loss: 0.950470\n",
      "(Iteration 21041 / 24460) loss: 1.143485\n",
      "(Iteration 21061 / 24460) loss: 1.077147\n",
      "(Iteration 21081 / 24460) loss: 0.896111\n",
      "(Iteration 21101 / 24460) loss: 0.940637\n",
      "(Iteration 21121 / 24460) loss: 1.007318\n",
      "(Iteration 21141 / 24460) loss: 1.023476\n",
      "(Iteration 21161 / 24460) loss: 1.095456\n",
      "(Iteration 21181 / 24460) loss: 0.950299\n",
      "(Iteration 21201 / 24460) loss: 1.056925\n",
      "(Iteration 21221 / 24460) loss: 1.069449\n",
      "(Iteration 21241 / 24460) loss: 0.930335\n",
      "(Iteration 21261 / 24460) loss: 1.018772\n",
      "(Iteration 21281 / 24460) loss: 1.068299\n",
      "(Iteration 21301 / 24460) loss: 1.035615\n",
      "(Iteration 21321 / 24460) loss: 1.091287\n",
      "(Iteration 21341 / 24460) loss: 0.963149\n",
      "(Iteration 21361 / 24460) loss: 0.953681\n",
      "(Iteration 21381 / 24460) loss: 1.288552\n",
      "(Iteration 21401 / 24460) loss: 1.106612\n",
      "(Iteration 21421 / 24460) loss: 0.925630\n",
      "(Iteration 21441 / 24460) loss: 1.011891\n",
      "(Iteration 21461 / 24460) loss: 0.965011\n",
      "(Iteration 21481 / 24460) loss: 0.818149\n",
      "(Iteration 21501 / 24460) loss: 1.033039\n",
      "(Iteration 21521 / 24460) loss: 0.909476\n",
      "(Iteration 21541 / 24460) loss: 1.001040\n",
      "(Iteration 21561 / 24460) loss: 1.010531\n",
      "(Iteration 21581 / 24460) loss: 1.032969\n",
      "(Iteration 21601 / 24460) loss: 0.893847\n",
      "(Iteration 21621 / 24460) loss: 0.910541\n",
      "(Iteration 21641 / 24460) loss: 1.023789\n",
      "(Iteration 21661 / 24460) loss: 0.952187\n",
      "(Iteration 21681 / 24460) loss: 1.053918\n",
      "(Iteration 21701 / 24460) loss: 0.739865\n",
      "(Iteration 21721 / 24460) loss: 0.891961\n",
      "(Iteration 21741 / 24460) loss: 0.972883\n",
      "(Iteration 21761 / 24460) loss: 1.123074\n",
      "(Iteration 21781 / 24460) loss: 1.097077\n",
      "(Iteration 21801 / 24460) loss: 0.943170\n",
      "(Iteration 21821 / 24460) loss: 1.067989\n",
      "(Iteration 21841 / 24460) loss: 0.946845\n",
      "(Iteration 21861 / 24460) loss: 0.883327\n",
      "(Iteration 21881 / 24460) loss: 1.072923\n",
      "(Iteration 21901 / 24460) loss: 0.947899\n",
      "(Iteration 21921 / 24460) loss: 1.012109\n",
      "(Iteration 21941 / 24460) loss: 1.148623\n",
      "(Iteration 21961 / 24460) loss: 0.868122\n",
      "(Iteration 21981 / 24460) loss: 0.925775\n",
      "(Iteration 22001 / 24460) loss: 1.138566\n",
      "(Epoch 18 / 20) train acc: 0.739000; val_acc: 0.655000\n",
      "(Iteration 22021 / 24460) loss: 0.912006\n",
      "(Iteration 22041 / 24460) loss: 0.811229\n",
      "(Iteration 22061 / 24460) loss: 0.965356\n",
      "(Iteration 22081 / 24460) loss: 0.924889\n",
      "(Iteration 22101 / 24460) loss: 1.004426\n",
      "(Iteration 22121 / 24460) loss: 1.018337\n",
      "(Iteration 22141 / 24460) loss: 0.974854\n",
      "(Iteration 22161 / 24460) loss: 1.189600\n",
      "(Iteration 22181 / 24460) loss: 0.807168\n",
      "(Iteration 22201 / 24460) loss: 0.913105\n",
      "(Iteration 22221 / 24460) loss: 1.197313\n",
      "(Iteration 22241 / 24460) loss: 0.967178\n",
      "(Iteration 22261 / 24460) loss: 0.925176\n",
      "(Iteration 22281 / 24460) loss: 0.987426\n",
      "(Iteration 22301 / 24460) loss: 1.097462\n",
      "(Iteration 22321 / 24460) loss: 0.977715\n",
      "(Iteration 22341 / 24460) loss: 0.845273\n",
      "(Iteration 22361 / 24460) loss: 1.209875\n",
      "(Iteration 22381 / 24460) loss: 0.965679\n",
      "(Iteration 22401 / 24460) loss: 0.981555\n",
      "(Iteration 22421 / 24460) loss: 0.890439\n",
      "(Iteration 22441 / 24460) loss: 1.123577\n",
      "(Iteration 22461 / 24460) loss: 1.034118\n",
      "(Iteration 22481 / 24460) loss: 0.913686\n",
      "(Iteration 22501 / 24460) loss: 0.889586\n",
      "(Iteration 22521 / 24460) loss: 1.036492\n",
      "(Iteration 22541 / 24460) loss: 0.821065\n",
      "(Iteration 22561 / 24460) loss: 0.814652\n",
      "(Iteration 22581 / 24460) loss: 1.003390\n",
      "(Iteration 22601 / 24460) loss: 0.918367\n",
      "(Iteration 22621 / 24460) loss: 1.137092\n",
      "(Iteration 22641 / 24460) loss: 1.051695\n",
      "(Iteration 22661 / 24460) loss: 0.975818\n",
      "(Iteration 22681 / 24460) loss: 0.818834\n",
      "(Iteration 22701 / 24460) loss: 0.998922\n",
      "(Iteration 22721 / 24460) loss: 1.039550\n",
      "(Iteration 22741 / 24460) loss: 0.888914\n",
      "(Iteration 22761 / 24460) loss: 0.968148\n",
      "(Iteration 22781 / 24460) loss: 0.734471\n",
      "(Iteration 22801 / 24460) loss: 0.788301\n",
      "(Iteration 22821 / 24460) loss: 1.228171\n",
      "(Iteration 22841 / 24460) loss: 1.036166\n",
      "(Iteration 22861 / 24460) loss: 0.970328\n",
      "(Iteration 22881 / 24460) loss: 1.068299\n",
      "(Iteration 22901 / 24460) loss: 0.924555\n",
      "(Iteration 22921 / 24460) loss: 1.044429\n",
      "(Iteration 22941 / 24460) loss: 1.010795\n",
      "(Iteration 22961 / 24460) loss: 1.064677\n",
      "(Iteration 22981 / 24460) loss: 0.960362\n",
      "(Iteration 23001 / 24460) loss: 0.818403\n",
      "(Iteration 23021 / 24460) loss: 0.716439\n",
      "(Iteration 23041 / 24460) loss: 1.070523\n",
      "(Iteration 23061 / 24460) loss: 1.140401\n",
      "(Iteration 23081 / 24460) loss: 1.080045\n",
      "(Iteration 23101 / 24460) loss: 0.851951\n",
      "(Iteration 23121 / 24460) loss: 0.831697\n",
      "(Iteration 23141 / 24460) loss: 0.883531\n",
      "(Iteration 23161 / 24460) loss: 1.261444\n",
      "(Iteration 23181 / 24460) loss: 1.049552\n",
      "(Iteration 23201 / 24460) loss: 0.904911\n",
      "(Iteration 23221 / 24460) loss: 0.915234\n",
      "(Epoch 19 / 20) train acc: 0.735000; val_acc: 0.651000\n",
      "(Iteration 23241 / 24460) loss: 0.826024\n",
      "(Iteration 23261 / 24460) loss: 1.014755\n",
      "(Iteration 23281 / 24460) loss: 1.100332\n",
      "(Iteration 23301 / 24460) loss: 1.076190\n",
      "(Iteration 23321 / 24460) loss: 0.938992\n",
      "(Iteration 23341 / 24460) loss: 0.908913\n",
      "(Iteration 23361 / 24460) loss: 1.157090\n",
      "(Iteration 23381 / 24460) loss: 0.867933\n",
      "(Iteration 23401 / 24460) loss: 1.172429\n",
      "(Iteration 23421 / 24460) loss: 1.057374\n",
      "(Iteration 23441 / 24460) loss: 1.045948\n",
      "(Iteration 23461 / 24460) loss: 0.863476\n",
      "(Iteration 23481 / 24460) loss: 1.073397\n",
      "(Iteration 23501 / 24460) loss: 1.167553\n",
      "(Iteration 23521 / 24460) loss: 0.887552\n",
      "(Iteration 23541 / 24460) loss: 0.881190\n",
      "(Iteration 23561 / 24460) loss: 0.915276\n",
      "(Iteration 23581 / 24460) loss: 1.083167\n",
      "(Iteration 23601 / 24460) loss: 0.973142\n",
      "(Iteration 23621 / 24460) loss: 0.939102\n",
      "(Iteration 23641 / 24460) loss: 0.906110\n",
      "(Iteration 23661 / 24460) loss: 1.033036\n",
      "(Iteration 23681 / 24460) loss: 0.976433\n",
      "(Iteration 23701 / 24460) loss: 0.842401\n",
      "(Iteration 23721 / 24460) loss: 0.898538\n",
      "(Iteration 23741 / 24460) loss: 1.107159\n",
      "(Iteration 23761 / 24460) loss: 0.921357\n",
      "(Iteration 23781 / 24460) loss: 0.987073\n",
      "(Iteration 23801 / 24460) loss: 0.945137\n",
      "(Iteration 23821 / 24460) loss: 0.801223\n",
      "(Iteration 23841 / 24460) loss: 0.954122\n",
      "(Iteration 23861 / 24460) loss: 1.047469\n",
      "(Iteration 23881 / 24460) loss: 1.045736\n",
      "(Iteration 23901 / 24460) loss: 1.012967\n",
      "(Iteration 23921 / 24460) loss: 0.908905\n",
      "(Iteration 23941 / 24460) loss: 0.834951\n",
      "(Iteration 23961 / 24460) loss: 0.936099\n",
      "(Iteration 23981 / 24460) loss: 1.073113\n",
      "(Iteration 24001 / 24460) loss: 1.166132\n",
      "(Iteration 24021 / 24460) loss: 1.221921\n",
      "(Iteration 24041 / 24460) loss: 0.931706\n",
      "(Iteration 24061 / 24460) loss: 1.238801\n",
      "(Iteration 24081 / 24460) loss: 0.991921\n",
      "(Iteration 24101 / 24460) loss: 1.008813\n",
      "(Iteration 24121 / 24460) loss: 1.097185\n",
      "(Iteration 24141 / 24460) loss: 0.880008\n",
      "(Iteration 24161 / 24460) loss: 0.921586\n",
      "(Iteration 24181 / 24460) loss: 0.920023\n",
      "(Iteration 24201 / 24460) loss: 1.062129\n",
      "(Iteration 24221 / 24460) loss: 0.970530\n",
      "(Iteration 24241 / 24460) loss: 1.058508\n",
      "(Iteration 24261 / 24460) loss: 0.894139\n",
      "(Iteration 24281 / 24460) loss: 1.013689\n",
      "(Iteration 24301 / 24460) loss: 1.112764\n",
      "(Iteration 24321 / 24460) loss: 1.039305\n",
      "(Iteration 24341 / 24460) loss: 0.895802\n",
      "(Iteration 24361 / 24460) loss: 0.959915\n",
      "(Iteration 24381 / 24460) loss: 1.231020\n",
      "(Iteration 24401 / 24460) loss: 0.997188\n",
      "(Iteration 24421 / 24460) loss: 0.961145\n",
      "(Iteration 24441 / 24460) loss: 0.923600\n",
      "(Epoch 20 / 20) train acc: 0.740000; val_acc: 0.661000\n",
      "Best validation accuracy: 0.663\n"
     ]
    }
   ],
   "source": [
    "def augment_data(X, y):\n",
    "    \"\"\"Augments CIFAR-10 data with horizontal flips and random crops.\"\"\"\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        img = X[i].transpose(1, 2, 0)  # Convert to HWC format\n",
    "\n",
    "        # Horizontal flip\n",
    "        if np.random.rand() > 0.5:\n",
    "            img_flipped = np.fliplr(img)\n",
    "            X_aug.append(img_flipped.transpose(2, 0, 1))\n",
    "            y_aug.append(y[i])\n",
    "\n",
    "        # Random crop\n",
    "        crop_size = 32\n",
    "        x_offset = np.random.randint(0, 4)\n",
    "        y_offset = np.random.randint(0, 4)\n",
    "\n",
    "        img_padded = np.pad(img, ((4, 4), (4, 4), (0, 0)), mode='constant')\n",
    "        img_cropped = img_padded[y_offset:y_offset + crop_size, x_offset:x_offset + crop_size, :]\n",
    "        X_aug.append(img_cropped.transpose(2, 0, 1))\n",
    "        y_aug.append(y[i])\n",
    "\n",
    "    X_aug = np.concatenate([X, np.array(X_aug)], axis=0)\n",
    "    y_aug = np.concatenate([y, np.array(y_aug)], axis=0)\n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Augment training data\n",
    "X_train_aug, y_train_aug = augment_data(data[\"X_train\"], data[\"y_train\"])\n",
    "\n",
    "# Create augmented data dictionary\n",
    "data_aug = {\n",
    "    'X_train': X_train_aug,\n",
    "    'y_train': y_train_aug,\n",
    "    'X_val': data[\"X_val\"],\n",
    "    'y_val': data[\"y_val\"],\n",
    "    'X_test': data[\"X_test\"],\n",
    "    'y_test': data[\"y_test\"]\n",
    "}\n",
    "\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "#   Implement a CNN to achieve greater than 65% validation accuracy\n",
    "#   on CIFAR-10.\n",
    "# ================================================================ #\n",
    "\n",
    "# Define the model architecture\n",
    "model = ThreeLayerConvNet(input_dim=(3, 32, 32), num_filters=64, filter_size=3,\n",
    "                        hidden_dim=512, num_classes=10, weight_scale=0.001, reg=0.001, use_batchnorm=True)\n",
    "\n",
    "# Define the solver parameters\n",
    "solver = Solver(model, data_aug, #use augmented data\n",
    "                num_epochs=20, batch_size=100,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 0.001,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "\n",
    "# Train the model\n",
    "solver.train()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print('Best validation accuracy:', solver.best_val_acc)\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32) \n",
      "y_train: (49000,) \n",
      "X_val: (1000, 3, 32, 32) \n",
      "y_val: (1000,) \n",
      "X_test: (1000, 3, 32, 32) \n",
      "y_test: (1000,) \n",
      "[1,   200] loss: 1.321\n",
      "[1,   400] loss: 1.021\n",
      "[1,   600] loss: 0.936\n",
      "[2,   200] loss: 0.721\n",
      "[2,   400] loss: 0.720\n",
      "[2,   600] loss: 0.664\n",
      "[3,   200] loss: 0.516\n",
      "[3,   400] loss: 0.532\n",
      "[3,   600] loss: 0.563\n",
      "[4,   200] loss: 0.380\n",
      "[4,   400] loss: 0.409\n",
      "[4,   600] loss: 0.427\n",
      "[5,   200] loss: 0.273\n",
      "[5,   400] loss: 0.292\n",
      "[5,   600] loss: 0.321\n",
      "[6,   200] loss: 0.190\n",
      "[6,   400] loss: 0.194\n",
      "[6,   600] loss: 0.240\n",
      "[7,   200] loss: 0.136\n",
      "[7,   400] loss: 0.155\n",
      "[7,   600] loss: 0.172\n",
      "[8,   200] loss: 0.092\n",
      "[8,   400] loss: 0.110\n",
      "[8,   600] loss: 0.131\n",
      "[9,   200] loss: 0.084\n",
      "[9,   400] loss: 0.100\n",
      "[9,   600] loss: 0.114\n",
      "[10,   200] loss: 0.073\n",
      "[10,   400] loss: 0.067\n",
      "[10,   600] loss: 0.094\n",
      "[11,   200] loss: 0.063\n",
      "[11,   400] loss: 0.065\n",
      "[11,   600] loss: 0.086\n",
      "[12,   200] loss: 0.050\n",
      "[12,   400] loss: 0.055\n",
      "[12,   600] loss: 0.064\n",
      "[13,   200] loss: 0.054\n",
      "[13,   400] loss: 0.055\n",
      "[13,   600] loss: 0.074\n",
      "[14,   200] loss: 0.047\n",
      "[14,   400] loss: 0.051\n",
      "[14,   600] loss: 0.066\n",
      "[15,   200] loss: 0.042\n",
      "[15,   400] loss: 0.038\n",
      "[15,   600] loss: 0.052\n",
      "[16,   200] loss: 0.043\n",
      "[16,   400] loss: 0.041\n",
      "[16,   600] loss: 0.050\n",
      "[17,   200] loss: 0.048\n",
      "[17,   400] loss: 0.037\n",
      "[17,   600] loss: 0.047\n",
      "[18,   200] loss: 0.038\n",
      "[18,   400] loss: 0.035\n",
      "[18,   600] loss: 0.043\n",
      "[19,   200] loss: 0.035\n",
      "[19,   400] loss: 0.037\n",
      "[19,   600] loss: 0.036\n",
      "[20,   200] loss: 0.030\n",
      "[20,   400] loss: 0.034\n",
      "[20,   600] loss: 0.036\n",
      "Finished Training\n",
      "Accuracy of the network on the 1000 test images: 76.60 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from cs231n.data_utils import get_CIFAR10_data  # Assuming this is where your data loading is\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "X_train = torch.from_numpy(data['X_train']).float().to(device)\n",
    "y_train = torch.from_numpy(data['y_train']).long().to(device)\n",
    "X_test = torch.from_numpy(data['X_test']).float().to(device)\n",
    "y_test = torch.from_numpy(data['y_test']).long().to(device)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define the 3-layer CNN with Batch Normalization\n",
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.relu4(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "net = ThreeLayerConvNet().to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 1000 test images: {100 * correct / total:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
